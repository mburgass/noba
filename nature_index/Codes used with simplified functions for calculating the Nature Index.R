library(MASS)
library(gamlss.dist)
library(truncnorm)


# Functions for calculating sum of squared differences between "observed" parameter values and 
# 	model distribution parameters

# 	Functions qdev.TNO, qdev.LOGNO, qdev.WEI, qdev.ZEXP, qdev.GA, qdev.PO, qdev.NBII, qdev.ZIP below 
# 	returns the sum of squared differences between "observed" values for the expected value (obs[2]) and two 
# 	quantiles (obs[1] and obs[3]) from an empirical distribution and ditto (m/q2, q1 and q3) predicted from 
# 	respectively a given truncated normal-, lognormal-, weibull-, zero-inflated exponential distribution
# 	gamma-, poisson-, negative binomial-, and zero-inflated poisson distribution. The lower bound in the truncated 
#	normal distribution is always zero, while the upper bound is infinity. With these restrictions all theoretical
# 	distributions have two parameters, except the Poisson with only one parameter.
#
# 	Input arguments:
# 	par	double	length=2	parameter values for theoretical distribution
# 	obs	double	length=3	observed mean and quantiles
# 	prob	double	length=2	"vector of confidence", i.e. proba=(p(rand.obs < q1),p(rand.obs < q2))
#
# 	Output:
# 	unnamed object	double	length=2	residual sums of squares 	
#
# 	Functions called:
# 	qtnorm, etruncnorm in the truncnorm package
#	qLOGNO, qWEI, qGA, qPO, qNBII, qZIP quantile functions in the gamlss.dist package
#	qexp quantile function from the stats package
#
# 	All functions programmed by Nigel Yoccoz except qdev.ZEXP programmed by Bård Pedersen
#
#
# 	Continuous distributions

# 	Truncated Normal distribution

qdev.TNO <- function(par,obs,prob) {
	q1<-qtnorm(p=prob[1], mean=par[1], sd=par[2], lower=0)
	q2<-etruncnorm(a=0, b=1e11, mean=par[1], sd=par[2])
	q3<-qtnorm(p=prob[2], mean=par[1], sd=par[2], lower=0)
	qqq<-c(q1,q2,q3)
	sum((qqq-obs)^2)
}

# 	Lognormal distribution

qdev.LOGNO <- function(par,obs,prob) {
	q1<-qLOGNO(p=prob[1],mu=par[1],sigma=par[2])
	m<-(exp(par[2]^2))^0.5*exp(par[1])
	q3<-qLOGNO(p=prob[2],mu=par[1],sigma=par[2])
	qmq<-c(q1,m,q3)
	sum((qmq-obs)^2)
}

# 	Weibull distribution
# 	Function modified by Bård Pedersen 03.04.2012
# 	in order to supress warnings generated by the gamma-function.

qdev.WEI <- function(par,obs,prob) {
	op <- options("warn")
	options(warn = -1)
	q1<-qWEI(p=prob[1],mu=par[1],sigma=par[2])
	m<-par[1]*gamma(1/par[2]+1)
	q3<-qWEI(p=prob[2],mu=par[1],sigma=par[2])
	options(op)
	qmq<-c(q1,m,q3)
	sum((qmq-obs)^2)
}

#	Zero-inflated exponential distribution

qdev.ZEXP <- function(par,obs,prob) {
	if (par[1] == 1) {
		qmq <- c(0.0,0.0,0.0)
	} else {
		q1<-ifelse(par[1] >= prob[1], 0, qexp(p=(prob[1]-par[1])/(1-par[1]),rate=par[2])) 
		m<-(1-par[1])/par[2]
		q3<-ifelse(par[1] >= prob[2], 0, qexp(p=(prob[2]-par[1])/(1-par[1]),rate=par[2]))
		qmq<-c(q1,m,q3)
	}
	sum((qmq-obs)^2)
}

# 	Gamma distribution

qdev.GA <- function(par,obs,prob) {
	q1<-qGA(p=prob[1],mu=par[1],sigma=par[2])
	m<-par[1]
	q3<-qGA(p=prob[2],mu=par[1],sigma=par[2])
	qmq<-c(q1,m,q3)
	sum((qmq-obs)^2)
}

# 	Discrete  distributions
#
# 	Poisson distribution

qdev.PO <- function(par,obs,prob) {
	q1<-qPO(p=prob[1],mu=par[1])
	m<-par[1]
	q3<-qPO(p=prob[2],mu=par[1])
	qmq<-c(q1,m,q3)
	sum((qmq-obs)^2)
}

# 	Negative Binomial distribuiton

qdev.NBII <- function(par,obs,prob) {
	q1<-qNBII(p=prob[1],mu=par[1],sigma=par[2])
	m<-par[1]
	q3<-qNBII(p=prob[2],mu=par[1],sigma=par[2])
	qmq<-c(q1,m,q3)
	sum((qmq-obs)^2)
}

# 	Zero-inflated Poisson distribution

qdev.ZIP <- function(par,obs,prob) {
	q1<-qZIP(p=prob[1],mu=par[1],sigma=par[2])
	m<-(1-par[2])*par[1]
	q3<-qZIP(p=prob[2],mu=par[1],sigma=par[2])
	qmq<-c(q1,m,q3)
	sum((qmq-obs)^2)
}

estim.fct<-function(obsval=c(0.3,0.6,0.8),proba=c(0.25,0.75),type="continuous"){

#	Function for finding the model distribution that best fits to a set of observed distribution parameters.
# 	estim.fct selects, for both continuous and descrete cases, the distribution among a predetermined set
# 	of model distribution families that best fits to an expected value and two quantiles provided as arguments.
# 	The predetermined set of continuous distributions is {Truncated Normal, Lognormal, Gumbel, Weibull, Gamma}
# 	The predetermined set of descrete distributions is {Poisson, Negative binomial, Zero inflated Poisson}
#
# 	Input arguments:
# 	obsval	double	observed vector of expected value and quantiles, obsval=(q1,obs,q2) where obs is the expected value.
# 	proba		double 	"vector of confidence", i.e. proba=(p(rand.obs < q1),p(rand.obs < q2))
# 	type		character	flag indicating type of distribution, whether "continuous" or "discrete"
#
# 	Output:
# 	res	data.frame consisting of the following vectors of length 1
# 	[[1]]	res$distrib character 	selected family for model distribution ("TruncNormal","LogNormal","Gumbel","Weibull","Gamma","Poisson","NegBinom","ZIP")
# 	[[2]]	res$mu	double	parameter 1 of selected distribution, sometimes equal to the expected value depending on selected family of model distributions.
# 	[[3]]	res$sd 	double	parameter 2 of selected distribution, sometimes equal to the standard deviation depending on selected family of model distributions.
# 	[[4]]	res$crit 	double	sum of squared deviations between observed values and those predicted by the selected model distribution.
#
# 	Functions called:
# 	qdev.TNO	local function in present collection
# 	qdev.LOGNO	local function in present collection
# 	qdev.WEI	local function in present collection
# 	qdev.GA	local function in present collection
# 	qdev.PO	local function in present collection
# 	qdev.NBII	local function in present collection
# 	qdev.ZEXP	local function in present collection
# 	qdev.ZIP	local function in present collection
# 	nlminb	in stats package 	
#
# 	Function programmed by Nigel Yoccoz
#
# 	Modified by Bård Pedersen 03.11.2014
# 	Gumbel distribution removed as option in order to avoid negative values in bootstrap samples. 
#	The Gumbel is replaced by zero-inflated exponential distribution.


#	if(obsval[2]==0){obsval[2]<-0.0001}

	if (type=="continuous"){
#
# 		for each family of predetermined continuous model distributions, find parameter values that 
# 		gives the best fit to obsval using the least squares criterion.
#
		mini1=nlminb(start=c(0.5, 1), objective=qdev.LOGNO,lower=c(-1e6,0.001),prob=proba,obs=obsval)

		a <- try(nlminb(start=c(0.5, 1), objective=qdev.TNO,lower=c(-1e6,0.001),prob=proba,obs=obsval),silent=T)
		if (length(a)==1){
			mini2 <- mini1
			mini2$objective<-10e10
		} else {
			mini2 <- a
		}

#		mini2=nlminb(start=c(0.5, 1), objective=qdev.TNO,lower=c(-1e6,0.001),prob=proba,obs=obsval)
#		mini3=nlminb(start=c(0.5, 1), objective=qdev.GU,lower=c(0.001,0.001),prob=proba,obs=obsval)
#
# 		exclude Gumbel as possible model distribution when the best Gumbel distribution generates negative values
#	
#		if(min(rGU(999,mini3$par[1],mini3$par[2]))<0){mini3$objective<-10e10}

		mini3=nlminb(start=c(0.1, 1), objective=qdev.ZEXP,lower=c(0.0,0.001),upper = c(1,Inf),prob=proba,obs=obsval)
		mini4=nlminb(start=c(0.5, 1), objective=qdev.WEI,lower=c(0.001,0.001),prob=proba,obs=obsval)
		mini5=nlminb(start=c(0.5, 1), objective=qdev.GA,lower=c(0.001,0.001),prob=proba,obs=obsval)
		mini<-list(mini1,mini2,mini3,mini4,mini5)
#
#	 	select and store model with the best fit to obsval
#
		critlist<-c(mini1$objective, mini2$objective, mini3$objective, mini4$objective, mini5$objective)
		distrib<-c("LogNormal","TruncNormal","ZIExponential","Weibull","Gamma")
		mu<-c(mini1$par[1],mini2$par[1],mini3$par[1],mini4$par[1],mini5$par[1])
		sig<-c(mini1$par[2],mini2$par[2],mini3$par[2],mini4$par[2],mini5$par[2])
		sel<-critlist==min(critlist)
		res<-data.frame(distrib=distrib[sel],mu=mu[sel],sig=sig[sel],crit=critlist[sel])
	}

	if (type=="discrete"){
#
# 		for each family of predetermined descrete model distributions, find parameter values that 
# 		gives the best fit to obsval using the least squares criterion.
#
		mini1=nlminb(start=c(0.5), objective=qdev.PO,lower=c(0.001,0.001),prob=proba,obs=obsval)
		mini2=nlminb(start=c(0.5, 1), objective=qdev.NBII,lower=c(0.001,0.001),prob=proba,obs=obsval)
		mini3=nlminb(start=c(0.5, 1), objective=qdev.ZIP,lower=c(0.001,0.001),upper=c(10000,0.999),prob=proba,obs=obsval)	
		mini<-list(mini1,mini2,mini3)
#
#	 	select and store model with the best fit to obsval
#
		critlist<-c(mini1$objective, mini2$objective, mini3$objective)
		distrib<-c("Poisson","NegBinom","ZIP")
		mu<-c(mini1$par[1],mini2$par[1],mini3$par[1])
		sig<-c(NA,mini2$par[2],mini3$par[2])	#only one parameter in Poisson
		sel<-critlist==min(critlist)
		res<-data.frame(distrib=distrib[sel],mu=mu[sel],sig=sig[sel],crit=critlist[sel])
	}

	if(nrow(res)>1){res<-res[1,]}				#in case minimum is two or more equal least squares

	return(res)
}

estimlight.fct <- function(obsval=c(0.3,0.6,0.8),proba=c(0.25,0.75),type="continuous"){

# 	This is a simplified version of estim.fct which is called if estim.fct fails.
# 	Function that selects, for both continuous and descrete cases, the distribution among a predetermined set
# 	of model distribution families that best fits to a mean value and two quantiles provided as arguments.
# 	The predetermined set of continuous distributions is {Normal, Lognormal}
# 	The predetermined set of descrete distributions is {Poisson, Negative binomial}
#
# 	Input arguments:
# 	obsval	double	observed vector of expected value and quantiles, obsval=(q1,obs,q2) where obs is the expected value.
# 	proba		double 	"vector of confidence", i.e. proba=(p(rand.obs < q1),p(rand.obs < q2))
# 	type		character	flag indicating type of distribution, whether "continuous" or "discrete"
#
# 	Output:
# 	res	data.frame consisting of the following vectors of length 1
# 	[[1]]	res$distrib character 	selected family for model distribution ("Normal","LogNormal","Poisson","NegBinom")
# 	[[2]]	res$mu	double	parameter 1 of selected distribution, sometimes equal to the expected value depending on selected family of model distributions.
# 	[[3]]	res$sd 	double	parameter 2 of selected distribution, sometimes equal to the standard deviation depending on selected family of model distributions..
# 	[[4]]	res$crit 	double	sum of squared deviations between observed values and those predicted by the selected model distribution.
#
# 	Functions called:
# 	qdev.NO	local function in present collection
# 	qdev.LOGNO	local function in present collection
# 	qdev.PO	local function in present collection
# 	qdev.NBII	local function in present collection
# 	nlminb	in stats package 
#
# 	Function programmed by Nigel Yoccoz
#
# 	Modified by Bård Pedersen 03.11.2014
# 	Normal distribution removed as option in order to avoid negative values in bootstrap samples.

	if(obsval[2]==0){obsval[2]<-0.0001}

	if (type=="continuous"){
#
# 		for each family of predetermined continuous model distributions, find parameter values that 
# 		gives the best fit to obsval using the least squares criterion.
#
		mini2=nlminb(start=c(0.5, 1), objective=qdev.LOGNO,lower=c(-1e6,0.001),prob=proba,obs=obsval)
		mini<-list(mini1,mini2)
#
# 		select and store model with the best fit to obsval
#
		critlist<-c(mini2$objective)
		distrib<-c("LogNormal")
		mu<-c(mini2$par[1])
		sig<-c(mini2$par[2])
		sel<-c(T) # only lognormal distribution remains an option
		res<-data.frame(distrib=distrib[sel],mu=mu[sel],sig=sig[sel],crit=critlist[sel])
	}

	if (type=="discrete"){
#
# 		for each family of predetermined continuous model distributions, find parameter values that 
# 		gives the best fit to obsval using the least squares criterion.
#
		mini1=nlminb(start=c(0.5), objective=qdev.PO,lower=c(0.001,0.001),prob=proba,obs=obsval)
		mini2=nlminb(start=c(0.5, 1), objective=qdev.NBII,lower=c(0.001,0.001),prob=proba,obs=obsval)
		mini<-list(mini1,mini2)
#
# 		select and store model with the best fit to obsval
#
		critlist<-c(mini1$objective, mini2$objective)
		distrib<-c("Poisson","NegBinom")
		mu<-c(mini1$par[1],mini2$par[1])
		sig<-c(NA,mini2$par[2])
		sel<-critlist==min(critlist)
		res<-data.frame(distrib=distrib[sel],mu=mu[sel],sig=sig[sel],crit=critlist[sel])
	}

	if(nrow(res)>1){res<-res[1,]}

	return(res)
}

elicitate.fct<-function(Expected.value=c(1.0),Lower=c(0.75),Upper = c(1.25)){

# 	elicitate.fct fits probability distributions to a set of indicator observations by selecting for each
#	observation the distribution among a predetermined set of model distribution families that best fits
#	the observation. The function presumes that indicator observations are each given as three parameters of
#	a continuous probability distribution: expected value and the lower- and upper quartiles.
# 	The predetermined set of continuous distributions is {Truncated Normal, Lognormal, Weibull, Gamma, zero-inflated exponential}
#
# 	Input arguments:
# 	Expected.value	double	length=length(Expected.value)	vector of the indicator observations' expected values
# 	Lower			double 	length=length(Expected.value)	vector of the indicator observations' 0.25-quantiles (lower quartiles)
# 	Upper			double	length=length(Expected.value)	vector of the indicator observations' 0.75-quantiles (upper quartiles
#
# 	Output:
# 	elicitation.results data.frame consisting of the following vectors
# 	[[1]]	elicitation.results$FK_DistID	character 	length=length(Expected.value)	vector of names of fitted model distributions
# 	[[2]]	elicitation.results$mu		double	length=length(Expected.value)	vector of first parameter of fitted model distributions
# 	[[3]]	elicitation.results$sig		double	length=length(Expected.value)	vector of second parameter of fitted model distributions
# 	[[4]]	elicitation.results$ssq 	double	length=length(Expected.value)	vector of sum of squared deviations between observed parameters 
#															and those of the fitted model distributions.
#
# 	Functions called:
# 	estim.fct		local function in present collection
# 	estimlight.fct	local function in present collection
#
# 	Expected.value, Lower and Upper should be of equal length.
# 	For indicator observations and reference values with no uncertainty, Expected.value, Lower, and Upper
# 	should be equal.
# 	Expected.value, Lower and Upper should be nonnegative. Negative values in an element of any of the these 
# 	vectors are interpreted as missing value and the corresponding output elements are set to NA.
#	Also, if one of Expected.value, Lower and Upper is NA, all corresponding output elements are set to NA
#
#	elicitation.fct first try to fit a model distribution with a call to the function estim.fct.
#	If this result in an error, it calls estimlight.fct instead.
#
#	Function programmed by Bård Pedersen

	N.values <- length(Expected.value)
	type.t <- "continuous" 	# All observations are intepreted as being measured on a continuous scale.
	prob.quant <- c(0.25,0.75) # Lower and Upper are interpreted as the lower and upper quartiles respectively
	elicitation.results <- data.frame(rep(NA,N.values),rep(NA,N.values),rep(NA,N.values),rep(NA,N.values))
	dimnames(elicitation.results) [[2]] <- c("FK_DistID","mu","sig","ssq")
	Expected.value[(Expected.value < 0 | Lower < 0 | Upper < 0 | is.na(Expected.value) | is.na(Lower) | is.na(Upper))] <- NA

	for (i in which(!is.na(Expected.value))) {
		obsval <- c(Lower[i],Expected.value[i],Upper[i])	
	#
	#	First, try to fit distribution model to obsval using estim.fct, if it fails - use estimlight.fct
	#
		a <- try(estim.fct(obsval=obsval,proba=prob.quant,type=type.t),silent=T)
		if (length(a)==1){a <- estimlight.fct(obsval=obsval,proba=prob.quant,type=type.t)}
		elicitation.results$FK_DistID[i] <- as.character(a$distrib)
		elicitation.results$mu[i]<-a$mu
		elicitation.results$sig[i]<-a$sig
		elicitation.results$ssq[i]<-a$crit
	}

	return(elicitation.results)
}

sampleobs.fct<-function(ValueID=1:2,Value=rep(1,2), DistID=rep(c("LogNormal"),2),mu=rep(0,2),sig=rep(1,2),nsim=100){

#	Function that returns nsim random draws from a set of distributions given by the vector DistID and
#	the parameter vectors mu and sig. The function presumes that the set only includes the
#	truncated normal-, lognormal-, weibull-, gamma-, zero-inflated exponential, poisson-, negative binomial-, 
#	and zero-inflated poisson distributions. The lower bound in the truncated normal distribution is always zero,
#	while the upper bound is infinity. With these restrictions all theoretical distributions have two parameters,
#	except the poisson with only one parameter.
#	Alternatively, if distrib == "NoBoot", the function returns nsim copies of the observation's expected value.
#
#	Input arguments:
#	ValueID		integer	length=length(ValueID)	vector of indicator observation IDs
#	DistID 		character	length=length(ValueID)	vector of distribution families ("TruncNormal","LogNormal","Weibull","ZIExponential","Gamma","Poisson","NegBinom","ZIP") or copy obs (when distrib == "NoBoot")
#	mu			double	length=length(ValueID)	vector of parameter 1 of model distributions
#	sig			double	length=length(ValueID)	vector of parameter 2 of model distributions
#	Value			double	length=length(ValueID)	vector of observed expected value to be copied if distrib == "NoBoot".
#	nsim			integer	length=1			number of draws (simulations)
#
#	Output 
#	bootmat	double matrix	dim=length(ValueID) x nsim	nsim random draws from specified (by DistID, mu and sig) model or nsim copies of Value
#
#	Functions called:
# 	rtnorm 					random generation function in the truncnorm package
#	rLOGNO, rWEI, rGA, rPO, rNBII, rZIP random generation functions in the gamlss.dist package
#	runif and rexp 				random generation functions from the stats package

# 	Function programmed by Nigel Yoccoz
#
# 	Modified by Bård Pedersen 03.11.2014
# 	Zero-inflated exponential distribution included as additional model distribution.
# 	Gumbel and Normal distributions removed.
# 	Code for sampling from the ZIExp distribution added.
#	Function now deals with a set of distributions given as three vectors and returns a matrix of nsim draws
#	from each distribution.

	N.values <- length(ValueID)

	bootmat <- matrix(0.0,nrow=N.values,ncol=nsim)
	dimnames(bootmat)[[1]] <- ValueID

	for (i in 1:N.values) {
		distrib <- DistID[i]

		if (distrib=="Gamma"){
			vec<-rGA(nsim,mu=mu[i],sigma=sig[i])
			while(NaN%in%vec){vec[is.na(vec)]<-rGA(length(vec[is.na(vec)]),mu=mu[i],sigma=sig[i])}
		}

		if (distrib=="LogNormal")    {vec<-rLOGNO(nsim,mu=mu[i],sigma=sig[i])}
		if (distrib=="TruncNormal")  {vec<-rtnorm(nsim,mean=mu[i],sd=sig[i],lower=0)}
		if (distrib=="Weibull")      {vec<-rWEI(nsim,mu=mu[i],sigma=sig[i])}
		if (distrib=="ZIExponential"){
			vec <- rep(0,nsim)
			zero.val <- runif(nsim,0,1) < mu[i]
			vec[!zero.val] <-rexp(nsim - sum(zero.val),rate=sig[i])
		}
		if (distrib=="Poisson")      {vec<-rPO(nsim,mu=mu[i])}
		if (distrib=="NegBinom")     {vec<-rNBII(nsim,mu=mu[i],sigma=sig[i])}
		if (distrib=="ZIP")          {vec<-rZIP(nsim,mu=mu[i],sigma=sig[i])}
		if (distrib=="NoBoot")       {vec<-rep(Value[i],nsim)}

		bootmat[i,] <- vec

	}

	return(bootmat)
}

scaleobs.fct<-function(ValueID=1:2,FK_OmraadeID=rep(1,2),FK_IndicatorID=rep(1,2),FK_RefAarID=c(0,1),nsim=100,
	bootmat=matrix(1,nrow = 2,ncol = nsim),ref.value.code=0,IndicatorID=1,FK_Scalingmodel=1){

# 	scaleobs.fct scales a set of indicator observations according to chosen scaling model (LOW or MAX) and 
#	associated reference values.
#	Each indicator observation should be entered either as nsim random draws from distributions fitted to the 
#	original observations that were provided as expected values and upper and lower quartiles, or as nsim 
#	repetitions of the original expected values provided by experts.
#	In the same way, reference values should be entered either as nsim random draws from distributions fitted 
#	to the original reference values, or as nsim repetitions of the original expected values.
#	Reference values and indicator observations should be entered together in the same matrix bootmat, together
#	with a vector of length equal to the number of indicator observations and reference values with codes 
#	identifying reference values from indicator observations.
#	Further, vectors of the same length containing the observations/reference values area IDs and indicator IDs 
#	should be entered.
#	Scaling models are specific for each indicator. They should be entered as a vector of scaling model codes 
#	where 1 = LOW and 2 = MAX, together with a vector of associated indicator IDs of the same length.
#
#	Input arguments:
#	ValueID		integer		length=length(ValueID)		vector of indicator observation IDs
#	FK_OmraadeID	integer		length=length(ValueID)		vector of indicator area IDs
#	FK_IndicatorID	integer		length=length(ValueID)		vector of indicator IDs
#	FK_RefAarID		integer		length=length(ValueID)		vector of IDs for observation years / reference values
#	nsim			integer		length=1				number of draws (simulations) in the estimation of NI
#	bootmat		double matrix	dim=length(ValueID) x nsim	nsim random draws from fitted distributions or nsim copies of expected values
#	ref.value.code	integer		length=1				code for reference values used in FK_RefAarID
#	IndicatorID		integer		length=length(IndicatorID)	vector of indicator IDs
#	FK_Scalingmodel	integer		length=length(IndicatorID)	vector of scaling model IDs, 1 = LOW and 2 = MAX
#
#	Output 
#	scaled.bootmat	double matrix	dim=length(ValueID) x nsim	nsim scaled draws from fitted distributions 
#												or copies of expected values
#
#	Note that scaleobs.fct scaled.bootmat is of the same dimension as bootmat. This means that scaled.bootmat
# 	contains nsim draws of scaled reference values which all are equal to 1. These must be removed from
# 	scaled.bootmat before using NIcalculation.fct to calculate random draws of the nature index.
# 
#	Further, if the input bootmat matrix contains indicator observations for several years, so will the
#	scaled.bootmat. However, the index is calculated for one year, so the output matrix scaled.bootmat must be
#	split accordingly into a set of matrices, where each matrix contains draws of scaled indicator observations 
#	for one single year only.
#
#	Function programmed by Bård Pedersen

	Reference.values <- FK_RefAarID == ref.value.code
	refmat <- 0*bootmat
	valuemat <- bootmat
	N.values <- length(ValueID)

	for (i in 1:N.values) {
		refmat[i,] <- bootmat[Reference.values & FK_OmraadeID == FK_OmraadeID[i],]
	}

	scaled.bootmat <- valuemat/refmat
	dimnames(scaled.bootmat)[[1]] <- ValueID

	model.mat <- matrix(1,nrow=N.values,ncol=nsim)
	FK_Scalingmodel <- as.character(FK_Scalingmodel)
	for (i in 1:N.values) {
		model.mat[i,] <- rep(FK_Scalingmodel[IndicatorID == FK_IndicatorID[i]],nsim)
	}
	scaled.bootmat[model.mat == 2] <- (-1)*scaled.bootmat[model.mat == 2] + 2
	scaled.bootmat[scaled.bootmat < 0] <- 0.0
	scaled.bootmat[scaled.bootmat > 1] <- 1.0
	if (!(is.matrix(scaled.bootmat))) {scaled.bootmat <- t(matrix(scaled.bootmat))} 

	return(scaled.bootmat)
}



areaweights.fct <- function(Municipalities=as.character(1:10),Regions=as.character(rep(1:5,2)),Area.municipality=runif(10),Indicators=c("ind1","ind2")) {

#	Function that calculates relevant area weights (Wk) for chosen spatial resolution of the NI output.
#	I.e. areaweights.fct calculates Wk for a set of aggregated, nonoverlapping, spatial units each
#	consisting of one to many basic spatial units (municipalities).
#	In the Norwegian implementation Wks are weights based on the area the major habitat in question covers
#	in each basic spatial unit. An alternative is to use the total area of the spatial unit, etc. 
#	Wk is the relevant municipality area divided by the corresponding total area of the aggregate.
#	Municipalities outside the aggregate receive weights equal zero
#
#	Area weights are relevant when NI is calculated for aggregates of basic spatial units.
#
#	The weights are stored in a three-dimensional array to be included in the calculation of the nature index in
#	a subsequent call to NIcalculation.fct.
#	In the array weights are repeated for each indicator included in the current calculation of NI.
#
#	Input arguments:
#	Municipalities	character		length=length(Municipality)	vector of names of basic spatial units
#	Regions		character		length=length(Municipality)	vector of names of aggregated spatial units
#												that the corresponding municipality is a part of.
#	Area.municipality	double		length=length(Municipality)	vector of relevant basic spatial unit areas
#	Indicators		character		length=length(Indicators)	vector of the names of all indicators included
#												in the calculation
#
#	Output 
#	Weights.reg.area	double array	dim=length(Municipality) x length(Indicators) x length(unique(Regions))	
#												array of area weights. 
#
#	Function programmed by Bård Pedersen
#
	Municipalities <- as.character(Municipalities)
	Regions <- as.character(Regions)
	Aggregates <- unique(Regions)
	Indicators <- as.character(Indicators)
	N.reg <- length(Aggregates)
	N.kom <- length(Municipalities)
	N.ind <- length(Indicators)

	Weights.reg.area <- array(rep(0.0,N.kom * N.ind * N.reg),dim = c(N.kom,N.ind,N.reg))
		dimnames(Weights.reg.area) [[1]] <- Municipalities
		dimnames(Weights.reg.area) [[2]] <- Indicators
		dimnames(Weights.reg.area) [[3]] <- Aggregates
	for (k in Aggregates) {
		Tot.area.reg <- sum(Area.municipality[Regions == k])
		Weights.reg.area[,,k] <- matrix(rep((Regions == k) * Area.municipality / Tot.area.reg,N.ind),ncol=N.ind)
	}

	return(Weights.reg.area)
}


munweights.fct <- function(
	Municipalities=c("Mun1","Mun2","Mun3","Mun4","Mun5","Mun6","Mun7","Mun8","Mun9","Mun10",
			     "Mun11","Mun12","Mun13","Mun14","Mun15","Mun16","Mun17","Mun18","Mun19","Mun20",
			     "Mun21","Mun22","Mun23","Mun24","Mun25","Mun26","Mun27","Mun28","Mun29","Mun30",
			     "Mun31","Mun32","Mun33","Mun34","Mun35","Mun36"),
	Indicators=c("Ind1","Ind2","Ind3","Ind4","Ind5","Ind6"),
	FK_TrophicgroupID=c(1,1,2,2,0,0),
	Key.indicators=c(F,F,F,F,T,T),
	Fidelity=rep(1,6),
	Areaind.Name=1:36,
	Areaind.Indicator=rep(c("Ind1","Ind2","Ind3","Ind4","Ind5","Ind6"),each=6),
	Area.Name=rep(1:36,each=5),
	Area.Municipality=Municipalities[c(sample(1:36,30),sample(1:36,30),sample(1:36,30),sample(1:36,30),sample(1:36,30),sample(1:36,30))]) {

#	Function that calculates fidelity weights (Wi), trophic weights (Wf) and their product for each combination
#	of municipality and indicator.
#	Wi is the fidelity of indicator to ecosystem in questions divided by summed fidelity 
#	of indicators belonging to the same trophic group occuring in a municipality.
#	Wf for non-key indicators equals 0.5/number of trophic groups observed in the municipality if also
#	key indicators are observed in the municipality, and 1/number of trophic groups observed in the municipality
#	if no key elements are observed in the municipality. Wf for key indicators equals 0.5 if also
#	non-key indicators are observed in the municipality, and 1 if only key indicators are observed.
#	Weights.trof : Matrix Double (N.kom x N.ind) Wi * Wf
#
#	Input arguments:
#	Municipalities	character		length=length(Municipality)	vector of names of basic spatial units
#	Indicators		character		length=length(Indicators)	vector of the names of all indicators included
#												in the calculation
#	FK_TrophicgroupID	integer		length=length(Indicators)	vector of trophic group IDs that the corresponding 
#												indicator belongs to
#	Key.indicators	logical		length=length(Indicators)	vector indicating whether corresponding indicator
#												is a key indicator or not
#	Fidelity		double		length=length(Indicators)	vector of indicator fidelities to the major 
#												habitat in question
#	Areaind.Name	integer		length=length(Areaind.Name)	vector of unique indicator area IDs
#	Areaind.Indicator character		length=length(Areaind.Name)	vector of indicator names corresponding to 
#												the entries in Areaind.Name
#	Area.Name		integer		length=length(Area.Name)	vector of indicator area IDs, where each indicator
#												area ID is repeated for each spatial unit it consist of
#	Area.Municipality character		length=length(Area.Name)	vector of names of basic spatial units
#												corresponding to the entries in Area.Name
#
#	Output 
#	Weights.trof	double matrix	dim=length(Municipality) x length(Indicators) Wi * Wf. 
#
#	Function programmed by Bård Pedersen
#
	Municipalities <- as.character(Municipalities)
	Indicators <- as.character(Indicators)
	N.omr <- length(Areaind.Indicator)
	N.kom <- length(Municipalities)
	N.ind <- length(Indicators)
	trof.mis.code <- max(FK_TrophicgroupID)+1

	# Calculate presence-matrix - meaning: Identify all combinations of indicator and municipality included in the
	# data and calculation. Will be used in the calculation of fidelity weights as they depend on the fidelity  
	# of other indicators present in a municipality.
	# Presence : Matrix Logical (N.kom.akt.aar x N.ind.akt.aar)

	Presence <- matrix(FALSE,nrow=N.kom,ncol=N.ind,dimnames = list(Municipalities, Indicators))
	for (j in 1:N.omr) {
		for (k in which(Area.Name == Areaind.Name[j])) {
			Presence[as.character(Area.Municipality[k]),as.character(Areaind.Indicator[j])] <- TRUE
		} 
	}

	# Calculate Wi for each combination of municipality and indicator
	#
	# Fidelity.mat : Matrix Double (N.kom x N.ind) 
	#	Fidelity to ecosystem in question for all indicators present in the different municipalities. 
	#	rowSums(Fidelity.mat) is the total fidelity in each municipality
	#	rowSums(Fidelity.mat*Func.mat.j) is the total fidelity for trophic group j in each municipality where
	#		Func.mat.j is a 1/0 vector signaling whether trophic group j is present in the various municipalities.
	# Weights.fidel : Matrix Double (N.kom x N.ind)
	#	Wi for non-key indicators.
	#	Wi for non-key indicators are zero for key indicators and in municipalities where the indicator is not observed.
	#	rowSums(Weights.fidel) is the number of trophic groups observed in each municipality.
	# Weights.fidel.key : Matrix Double (N.kom x N.ind)
	#	Wi for key indicators.
	#	Wi for key indicators are zero for non-key indicators and in municipalities where the indicator is not observed.
	#	rowSums(Weights.fidel.key) is a 1/0 vector signaling whether any key indicators are observed in the different municipalities.
	# Func.group : Vector Integer (N.ind) 
	#	Recoding of Indikatorer$FK_FunksjonellGruppeID where trof.mis.code codes for key indicator.

	Fidelity.mat <- t(matrix(rep(Fidelity,N.kom),ncol=N.kom))*Presence
	dimnames(Fidelity.mat) <- dimnames(Presence)
	Weights.fidel <- Fidelity.mat*0.0
	Weights.fidel.key <- Weights.fidel
	Func.group <- FK_TrophicgroupID
	Func.group[Key.indicators] <- trof.mis.code
	Func.groups.present <- unique(Func.group[Func.group != trof.mis.code])
	N.Func.groups.present <- length(Func.groups.present) 

	for (j in Func.groups.present) {
		Weights.fidel[,Func.group == j] <- Fidelity.mat[,Func.group == j] / 
			rowSums(Fidelity.mat * t(matrix(rep(Func.group == j,N.kom),ncol=N.kom)))
	}
	Weights.fidel[is.na(Weights.fidel)] <- 0.0

	Weights.fidel.key[,Func.group == trof.mis.code] <- Fidelity.mat[,Func.group == trof.mis.code] / 
			rowSums(Fidelity.mat * t(matrix(rep(Func.group == trof.mis.code,N.kom),ncol=N.kom)))
	Weights.fidel.key[is.na(Weights.fidel.key)] <- 0.0

	# For each indicator and municipality, calculate weights determined by trophic group, key/nonkey indicator and fidelity.
	# Wf for non-key indicators equals 0.5/number of trophic groups observed in the municipality if also
	# key indicators are observed in the municipality, and 1/number of trophic groups observed in the municipality
	# if no key elements are observed in the municipality. Wf for key indicators equals 0.5 if also
	# non-key indicators are observed in the municipality, and 1 if only key indicators are observed.
	#
	# N.troph.groups : Matrix Double (N.kom x N.ind) NB!!Identical N.ind columns
	#	Number of trophic groups observed in each municipality.
	# Weights.trof.group : Matrix Double (N.kom x N.ind) NB!!Identical N.ind columns
	#	Wf for non-key indicators
	# Where.there.are.keys : Matrix Integer (N.kom x N.ind) NB!!Identical N.ind columns
	#	1/0 matrix signaling whether any key indicators are observed in the different municipalities.
	# Weights.key : Matrix Double (N.kom x N.ind) NB!!Identical N.ind columns
	# 	Wf for key indicators
	# Weights.trof : Matrix Double (N.kom x N.ind) Wi * Wf

	N.troph.groups <- matrix(rep(rowSums(Weights.fidel),N.ind),ncol=N.ind)
	Where.there.are.keys <- matrix(rep(rowSums(Weights.fidel.key),N.ind),ncol=N.ind)

	Weights.trof.group <- (1-(0.5*Where.there.are.keys))/N.troph.groups 
	Weights.trof.group[Weights.trof.group==Inf] <- 0.0

	Weights.key <- (1-(0.5*((N.troph.groups > 0)*1)))* Where.there.are.keys
	Weights.key[is.na(Weights.key)] <- 0.0

	Weights.trof <- ((Weights.trof.group*Weights.fidel) + (Weights.key*Weights.fidel.key))

	return(Weights.trof)
}

#########################################################################################################

NIcalculation.fct <- function(
	Areaind.Name=c(1,2),
	Areaind.Indicator=rep("Ind1",2),
	Area.Name=rep(1:2,each=5),
	Area.Municipality=c("Mun1","Mun2","Mun3","Mun4","Mun5","Mun6","Mun7","Mun8","Mun9","Mun10"),
	FK_OmraadeID=c(1,2),
	nsim=100,
	scaled.bootmat=matrix(1,nrow = 2,ncol = nsim,dimnames = list(c(1,2))),
	Weights.trof=matrix(1,nrow=10,ncol=1,dimnames = list(c("Mun1","Mun2","Mun3","Mun4","Mun5","Mun6","Mun7","Mun8","Mun9","Mun10"),c("Ind1"))),
	Weights.reg.area=NULL) {
	
#	Function for calculating nsim draws of NI from nsim sets of (N.omr) scaled observations for (N.ind) indicators and 
#	corresponding areaweights, trophic weights and fidelity weights for a set of (N.kom) basic spatial units
#	or a set of (N.reg) aggregated NI-areas each consisting of one or several basic spatial units.
#	Scaled indicatorobservations are calculated in scaleobs.fct, while area weights are calculated in areaweights.fct,
#	and the product of trophic weights and fidelity weights in munweights.fct.
#
#	NI is calculated for a set of NI-areas, while scaled indicator observations, on the other hand, refer to indicator
#	specific areas that may vary in geographical extent among indicators, and will in general not correspond to the NI-areas.
#	Thus, NIcalculation.fct needs input about the spatial extent of each indicator area and NI-area.
# 
#	Area weights should not be entered to NIcalculation.fct if NI is calculated for basic spatial units.
#
#	NIcalculation returns a matrix of nsim draws of NI for each NI-area (or basic spatial unit).
#
#	Input arguments:
#	Areaind.Name	integer		length=N.omr			vector of unique indicator area IDs
#	Areaind.Indicator character		length=N.omr			vector of indicator names corresponding to 
#												the entries in Areaind.Name
#	Area.Name		integer		length=length(Area.Name)	vector of indicator area IDs, where each indicator
#												area ID is repeated for each basic spatial unit it consist of
#	Area.Municipality character		length=length(Area.Name)	vector of names of corresponding basic spatial units
#	FK_OmraadeID	integer		length=N.omr			vector of indicator area IDs corresponding to the
#												sequence of indicator observations in scaled.bootmat
#	nsim			integer		length=1				number of draws (simulations) in the estimation of NI
#	scaled.bootmat	double matrix	dim=N.omr x nsim			nsim random draws of scaled indicator observations or 
#												nsim copies of scaled expected values
#	Weights.trof	double matrix	dim=N.kom x N.ind			trophic weights * fidelity weights
#	Weights.reg.area	double array	dim=N.kom x N.ind x N.reg	array of area weights
#
#	Output: 
#	NI			double matrix	dim=(N.kom or N.reg) x nsim 	matrix of NI draws 
#
#	Function programmed by Bård Pedersen
#

	N.kom <- dim(Weights.trof)[[1]]
	N.ind <- dim(Weights.trof)[[2]]
	N.omr <- length(Areaind.Indicator)

	Scaled.ind.per.mun <- array(rep(Weights.trof*0.0,nsim),dim = c(N.kom,N.ind,nsim))
		dimnames(Scaled.ind.per.mun) [[1]] <- dimnames(Weights.trof) [[1]]
		dimnames(Scaled.ind.per.mun) [[2]] <- dimnames(Weights.trof) [[2]]
		dimnames(Scaled.ind.per.mun) [[3]] <- 1:nsim

	for (j in 1:N.omr) {
		for (k in which(Area.Name == Areaind.Name[j])) {
			Scaled.ind.per.mun[as.character(Area.Municipality[k]),as.character(Areaind.Indicator[j]),] <- 
			scaled.bootmat[FK_OmraadeID == Areaind.Name[j],]
		}
	}

	if (length(Weights.reg.area) == 0) {
		NI <- matrix(0.0,nrow=N.kom,ncol=nsim)
		dimnames(NI) [[1]] <- dimnames(Weights.trof) [[1]]
		for (j in 1:nsim) {
			NI[,j] <- rowSums(Scaled.ind.per.mun[,,j] * Weights.trof)
		}
	} else {
		N.reg <- dim(Weights.reg.area)[3]
		Regions <- dimnames(Weights.reg.area) [[3]]
		NI <- matrix(0.0,nrow=N.reg,ncol=nsim)
		if (length(dimnames(Weights.reg.area) [[3]]) > 1) {
			dimnames(NI) [[1]] <- dimnames(Weights.reg.area) [[3]]
		} else {
			dimnames(NI) [[1]] <- list(dimnames(Weights.reg.area) [[3]])
		}
		for (j in 1:nsim) {
			for (k in Regions) {NI[k,j] <- sum(Scaled.ind.per.mun[,,j] * Weights.trof * Weights.reg.area[,,k])}
		}
	}

	return(NI)
}

NIestimation.fct <- function(NI=matrix(1,nrow=1,ncol=100),kvantiler=c(0.025,0.5,0.975)) {

# 	NIestimation.fct provides point estimates of the Nature Index together with confidence intervals for a set of
# 	N.reg basic spatial units or aggregated NI-areas from a N.reg x nsim matrix of nsim simulations of NI for each area.
# 	As default NIestimation returns the median simulated value as the point estimate and the 0.025- and 0.975-
#	quantiles as limits for the confidence interval.
#
#	Input arguments:
#	NI			double matrix	dim=(N.kom or N.reg) x nsim 	matrix of NI draws
#	kvantiler		double		length=3				Quantiles used as basis for point estimate and
#												limits of confidence interval.
#
#	Output: 
#	NIest			double matrix	dim=(N.kom or N.reg) x 3 	matrix of NI estimates 
#
#	Function programmed by Bård Pedersen
#
	if (dim(NI)[1] == 1) {
		NIest <- t(as.matrix(NI[,1:3]*0))
		dimnames(NIest)[[1]] <- list(dimnames(NI)[[1]])
	} else {
		NIest <- NI[,1:3]*0
	}
	dimnames(NIest)[[2]] <- as.character(kvantiler)
	for (j in 1:dim(NI)[1]) {
		NIest[j,] <- quantile(NI[j,],kvantiler)
	}
	return(NIest)
}



##########################

file.path.name.extdat2 <- "M:/My Documents/Naturindeks/Bulgaria/Scripts to Bulgarian experts/TestdataNI.RData"
load(file=file.path.name.extdat2)

rbind(head(Observations),tail(Observations))
Indic

Observations <- data.frame(
	Observations,
	elicitate.fct(
		Expected.value=Observations$Expected.value,
		Lower=Observations$Lower,
		Upper = Observations$Upper)[,1:3])

years <- unique(Observations$ReferenceYearID[Observations$ReferenceYearID!=0])
Alt_DistID <- Observations$FK_DistID
Alt_DistID[Observations$ReferenceYearID==0] <- "NoBoot"

bootmatall <- sampleobs.fct(
	ValueID=Observations$ValueID,
	Value=Observations$Expected.value, 
	DistID=Alt_DistID,
	mu=Observations$mu,
	sig=Observations$sig,
	nsim=1000)

scaled.bootmatall <- scaleobs.fct(
	ValueID=Observations$ValueID,
	FK_OmraadeID=Observations$IndicatorareaID,
	FK_IndicatorID=Observations$IndicatorID,
	FK_RefAarID=Observations$ReferenceYearID,
	nsim=1000,
	bootmat=bootmatall,
	ref.value.code=0,
	IndicatorID=Indic$IndicatorID,
	FK_Scalingmodel=Indic$Scalingmodel)

munweights <- munweights.fct(
	Municipalities=BSunits$Basicunit,
	Indicators=Indic$Indicator_name,
	FK_TrophicgroupID=Indic$TrophicgroupID,
	Key.indicators=Indic$Key.indicators,
	Fidelity=Indic$Fidelity,
	Areaind.Name=Indicator.area.ind$IndicatorareaID,
	Areaind.Indicator=Indicator.area.ind$Indicator_name,
	Area.Name=Indicator.area$IndicatorareaID,
	Area.Municipality=Indicator.area$Basicunit)

areaweights2 <- areaweights.fct(
	Municipalities=BSunits$Basicunit,
	Regions=BSunits$NIarea2,
	Area.municipality=BSunits$Area,
	Indicators=Indic$Indicator_name)

NIresult <- NULL

for (i in years) {

	scaled.bootmatyeari <- scaled.bootmatall[Observations$ReferenceYearID == i,]

	NIi <- NIestimation.fct(NIcalculation.fct(
		Areaind.Name=Indicator.area.ind$IndicatorareaID,
		Areaind.Indicator=Indicator.area.ind$Indicator_name,
		Area.Name=Indicator.area$IndicatorareaID,
		Area.Municipality=Indicator.area$Basicunit,
		FK_OmraadeID=Observations$IndicatorareaID[Observations$ReferenceYearID == i],
		nsim=1000,
		scaled.bootmat=scaled.bootmatyeari,
		Weights.trof=munweights,
		Weights.reg.area=areaweights2))

	year <- rep(i,dim(NIi) [1])
	iresult <- cbind(year,NIi)
	NIresult <- rbind(NIresult,iresult)

}

NIresult

sampleobs.fct(
	ValueID=Observations$ValueID,
	Value=Observations$Expected.value, 
	DistID=Observations$FK_DistID,
	mu=Observations$mu,
	sig=Observations$sig,
	nsim=10)[1:6,]

Alt_DistID <- Observations$FK_DistID


[1:10,]

Observationsyear2 <- Observations[Observations$ReferenceYearID %in% c(0,2),]
Alt_DistIDyear2 <- Alt_DistID[Observations$ReferenceYearID %in% c(0,2)]
sampleobs.fct(
	ValueID=Observationsyear2$ValueID,
	Value=Observationsyear2$Expected.value, 
	DistID=Alt_DistIDyear2,
	mu=Observationsyear2$mu,
	sig=Observationsyear2$sig,
	nsim=10)[1:10,]

bootmatyear2 <- sampleobs.fct(
	ValueID=Observationsyear2$ValueID,
	Value=Observationsyear2$Expected.value, 
	DistID=Alt_DistIDyear2,
	mu=Observationsyear2$mu,
	sig=Observationsyear2$sig,
	nsim=1000)

scaled.bootmatyear2 <- scaleobs.fct(
	ValueID=Observationsyear2$ValueID,
	FK_OmraadeID=Observationsyear2$IndicatorareaID,
	FK_IndicatorID=Observationsyear2$IndicatorID,
	FK_RefAarID=Observationsyear2$ReferenceYearID,
	nsim=1000,
	bootmat=bootmatyear2,
	ref.value.code=0,
	IndicatorID=Indic$IndicatorID,
	FK_Scalingmodel=Indic$Scalingmodel)[1:10,1:10]



areaweights1 <- areaweights.fct(
	Municipalities=BSunits$Basicunit,
	Regions=BSunits$NIarea1,
	Area.municipality=BSunits$Area,
	Indicators=Indic$Indicator_name)

areaweights2 <- areaweights.fct(
	Municipalities=BSunits$Basicunit,
	Regions=BSunits$NIarea2,
	Area.municipality=BSunits$Area,
	Indicators=Indic$Indicator_name)

areaweights.fct(
	Municipalities=BSunits$Basicunit,
	Regions=BSunits$NIarea2,
	Area.municipality=BSunits$Area/BSunits$Area,
	Indicators=Indic$Indicator_name) [,1:10,]

colSums(areaweights.fct(
	Municipalities=BSunits$Basicunit,
	Regions=BSunits$NIarea2,
	Area.municipality=BSunits$Area,
	Indicators=Indic$Indicator_name)) [1:10,]

munweights <- munweights.fct(
	Municipalities=BSunits$Basicunit,
	Indicators=Indic$Indicator_name,
	FK_TrophicgroupID=Indic$TrophicgroupID,
	Key.indicators=Indic$Key.indicators,
	Fidelity=Indic$Fidelity,
	Areaind.Name=Indicator.area.ind$IndicatorareaID,
	Areaind.Indicator=Indicator.area.ind$Indicator_name,
	Area.Name=Indicator.area$IndicatorareaID,
	Area.Municipality=Indicator.area$Basicunit)

munweights[,1:6]
rowSums(munweights)

rowSums(munweights.fct(
	Municipalities=BSunits$Basicunit,
	Indicators=Indic$Indicator_name,
	FK_TrophicgroupID=rep(4,length(Indic$TrophicgroupID)),
	Key.indicators=rep(F,length(Indic$Key.indicators)),
	Fidelity=rep(1.0,length(Indic$Fidelity)),
	Areaind.Name=Indicator.area.ind$IndicatorareaID,
	Areaind.Indicator=Indicator.area.ind$Indicator_name,
	Area.Name=Indicator.area$IndicatorareaID,
	Area.Municipality=Indicator.area$Basicunit)) [1:6]


dimnames(areaweights1)[[3]]


NIestimation.fct(NIcalculation.fct(
	Areaind.Name=Indicator.area.ind$IndicatorareaID,
	Areaind.Indicator=Indicator.area.ind$Indicator_name,
	Area.Name=Indicator.area$IndicatorareaID,
	Area.Municipality=Indicator.area$Basicunit,
	FK_OmraadeID=Observationsyear2$IndicatorareaID[Observationsyear2$ReferenceYearID != 0],
	nsim=1000,
	scaled.bootmat=scaled.bootmatyear2[Observationsyear2$ReferenceYearID != 0,],
	Weights.trof=munweights,
	Weights.reg.area=areaweights1))

NIestimation.fct(NIcalculation.fct(
	Areaind.Name=Indicator.area.ind$IndicatorareaID,
	Areaind.Indicator=Indicator.area.ind$Indicator_name,
	Area.Name=Indicator.area$IndicatorareaID,
	Area.Municipality=Indicator.area$Basicunit,
	FK_OmraadeID=Observationsyear2$IndicatorareaID[Observationsyear2$ReferenceYearID != 0],
	nsim=1000,
	scaled.bootmat=scaled.bootmatyear2[Observationsyear2$ReferenceYearID != 0,],
	Weights.trof=munweights,
	Weights.reg.area=areaweights2))

NIestimation.fct(NIcalculation.fct(
	Areaind.Name=Indicator.area.ind$IndicatorareaID,
	Areaind.Indicator=Indicator.area.ind$Indicator_name,
	Area.Name=Indicator.area$IndicatorareaID,
	Area.Municipality=Indicator.area$Basicunit,
	FK_OmraadeID=Observationsyear2$IndicatorareaID[Observationsyear2$ReferenceYearID != 0],
	nsim=1000,
	scaled.bootmat=scaled.bootmatyear2[Observationsyear2$ReferenceYearID != 0,],
	Weights.trof=munweights))

NIestimation.fct <- function(NI=matrix(1,nrow=1,ncol=100),kvantiler=c(0.025,0.5,0.975)) {

